{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from json_parser.py import JSONGraph, JSONEndpoint, JSONVertex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_news_links(date: str) -> list[str]:\n",
    "    \"\"\"Returns links to featured GroundNews articles from the front page of\n",
    "    GroundNews on the inputted date\"\"\"\n",
    "    try:\n",
    "        date = ''.join(date.split('-'))\n",
    "        if len(date) != 8:\n",
    "            raise ValueError('Date must be in YYYY-MM-DD format')\n",
    "    except ValueError:\n",
    "        raise ValueError('Date must be in YYYY-MM-DD format')\n",
    "    \n",
    "    url = f\"https://web.archive.org/web/{date}/https://ground.news/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionRefusedError('Connection Error')\n",
    "    content = BeautifulSoup(response.text, \"html.parser\")\n",
    "    anchors = content.find_all(\"a\", href=True)\n",
    "    anchor_links = [x.attrs['href'] for x in anchors] \n",
    "    article_links = [x for x in anchor_links if 'article/' in x]\n",
    "    article_links = [x[x.find('https:'):] for x in article_links]\n",
    "    return article_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "old_dates = None\n",
    "try:\n",
    "    scraped_df = pd.read_csv('data/complete_article_data.csv')\n",
    "    old_dates = set(scraped_df['date'].values.tolist())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#CHANGE HERE FOR NEW URLS (don't use same dates)\n",
    "dates = ['2025-02-01', '2025-01-01', '2024-12-01', '2024-11-01', '2024-10-01',\n",
    "        '2024-09-01', '2024-08-01', '2024-07-01', '2024-06-01', '2024-05-01',\n",
    "        '2024-04-01', '2024-03-01', '2024-02-01', '2024-01-01']\n",
    "\n",
    "links = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through the dates list, grabbing available ground news article links from the ground news homepage on the input date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, date in enumerate(dates):\n",
    "    if old_dates:\n",
    "        if date in old_dates:\n",
    "            print(f'Articles from {date} already scraped')\n",
    "            continue\n",
    "    links.append(get_ground_news_links(dates[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ground.news/article/ec825844-072e-47b7...</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ground.news/article/person-in-car-kill...</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ground.news/article/venezuela-agrees-t...</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ground.news/article/fatal-dc-aircraft-...</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ground.news/article/tulsi-gabbards-dni...</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date\n",
       "0  https://ground.news/article/ec825844-072e-47b7...  2025-02-01\n",
       "1  https://ground.news/article/person-in-car-kill...  2025-02-01\n",
       "2  https://ground.news/article/venezuela-agrees-t...  2025-02-01\n",
       "3  https://ground.news/article/fatal-dc-aircraft-...  2025-02-01\n",
       "4  https://ground.news/article/tulsi-gabbards-dni...  2025-02-01"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_col = []\n",
    "date_col = []\n",
    "for ind, link_group in enumerate(links):\n",
    "    date = dates[ind]\n",
    "    for link in link_group:\n",
    "        link_col.append(link)\n",
    "        date_col.append(date)\n",
    "\n",
    "gnews_link_df = pd.DataFrame()\n",
    "gnews_link_df['url'] = link_col\n",
    "gnews_link_df['date'] = date_col\n",
    "gnews_link_df.to_csv('data/ground_news_links.csv', index=False)\n",
    "gnews_link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews_link_df = pd.read_csv('data/ground_news_links.csv')\n",
    "\n",
    "urls = []\n",
    "biases = []\n",
    "outlets = []\n",
    "titles = []\n",
    "dates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through ground news article links, grabbing all available urls to outside articles as well as recording the outlet and bias that url is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "404\n",
      "404\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "for ind, url in enumerate(gnews_link_df['url'].values[400:]):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(response.status_code)\n",
    "            raise ConnectionRefusedError('Connection Error')\n",
    "    except:\n",
    "        continue\n",
    "    content = BeautifulSoup(response.text, \"html.parser\")\n",
    "    js =json.loads(content.find(type=\"application/json\").text)\n",
    "    graph = JSONGraph(js)\n",
    "    url_data = list(graph.find_tags('firstTenSources').values())[0]\n",
    "    date = gnews_link_df['date'].values[ind] \n",
    "    for url_dict in url_data:\n",
    "        biases.append(url_dict['sourceInfo']['bias'])\n",
    "        outlets.append(url_dict['sourceInfo']['name'])\n",
    "        urls.append(url_dict['url'])\n",
    "        titles.append(url_dict['title'])\n",
    "        dates.append(date)\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_queue_df = pd.DataFrame()\n",
    "scrape_queue_df['url'] = urls\n",
    "scrape_queue_df['title'] = titles\n",
    "scrape_queue_df['outlet'] = outlets\n",
    "scrape_queue_df['bias'] = biases\n",
    "scrape_queue_df['date'] = dates\n",
    "\n",
    "scrape_queue_df.head()\n",
    "scrape_queue_df.to_csv('data/articles_to_scrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=2.5)\n",
    "        if response.status_code != 200:\n",
    "            return 'Error'\n",
    "    except:\n",
    "        return 'Error'\n",
    "    content = BeautifulSoup(response.text, \"html.parser\")\n",
    "    all_p = content.find_all('p')\n",
    "    clean_text = [x.text for x in all_p]\n",
    "    full_text = ' '.join(clean_text)\n",
    "    return full_text\n",
    "\n",
    "scrape_queue_df = pd.read_csv('data/articles_to_scrape.csv')\n",
    "try:\n",
    "    pre_scraped_df = pd.read_csv('data/scraped_articles.csv')\n",
    "    scraped_urls = pre_scraped_df['url'].values.tolist()\n",
    "    scraped_text = pre_scraped_df['content'].values.tolist()\n",
    "except:\n",
    "    scraped_urls = []\n",
    "    scraped_text = []\n",
    "\n",
    "scraped_text_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterates through unseen URLs, scraping their text and occasionally updating the CSV\n",
    "for ind, url in enumerate(scrape_queue_df['url'].values):\n",
    "    if ind % 30 == 0:\n",
    "        print(ind, time.process_time())\n",
    "    if url not in scraped_urls:\n",
    "        scraped_text.append(scrape_url(url))\n",
    "        scraped_urls.append(url)\n",
    "    else:\n",
    "        continue\n",
    "    if ind % 30 == 0:\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['url'] = scraped_urls\n",
    "        temp_df['content'] = scraped_text\n",
    "        temp_df.to_csv('data/scraped_articles.csv', index=False)\n",
    "        scraped_text_df = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out URLS that could not be scraped from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_articles = []\n",
    "complete_urls = []\n",
    "for ind, article in enumerate(scraped_text_df['content'].values):\n",
    "    if len(article.split()) > 50:\n",
    "        complete_articles.append(article)\n",
    "        complete_urls.append(scraped_text_df['url'].values[ind])\n",
    "\n",
    "complete_df = pd.DataFrame()\n",
    "complete_df['url'] = complete_urls\n",
    "complete_df['content'] = complete_articles\n",
    "complete_df = scrape_queue_df.merge(complete_df, how='inner', on='url')\n",
    "complete_df.to_csv('data/complete_article_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON parser class if import did not work for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONGraph:\n",
    "\n",
    "    def __init__(self, js: dict):\n",
    "        self.js = js\n",
    "        self.roots = list(js.keys())\n",
    "        queue = [self.add_vert(item, None) for item in self.roots]\n",
    "\n",
    "        endpoints = {}\n",
    "        endpoint_parents = {}\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            node_path = node.get_path()\n",
    "            node_val = self.get_item(js, node_path)\n",
    "            #Handling endpoints\n",
    "            if type(node_val) == dict:\n",
    "                children = node_val.keys()\n",
    "                for child in children:\n",
    "                    queue.append(self.add_vert(child, node))                \n",
    "            elif type(node_val) == list:\n",
    "                for ind, item in enumerate(node_val):\n",
    "                    if type(item) == dict:\n",
    "                        children = item.keys()\n",
    "                        for child in children:\n",
    "                            queue.append(self.add_vert([ind, child], node))\n",
    "                    else: \n",
    "                        endpoint = self.add_endpoint([ind, item], node)\n",
    "                        endpoints[endpoint] = node_val         \n",
    "            else:\n",
    "                endpoint = self.add_endpoint(node_val, node)\n",
    "                endpoints[endpoint] = node_val\n",
    "        self.endpoints = endpoints\n",
    "        return\n",
    "\n",
    "    def add_vert(self, name, parent: \"JSONVertex | None\") -> \"JSONVertex\":\n",
    "        return JSONVertex(name, parent)\n",
    "\n",
    "    def add_endpoint(self, value: any, parent: \"JSONVertex\") -> \"JSONEndpoint\":\n",
    "        return JSONEndpoint(value, parent)\n",
    "\n",
    "    def get_id(self, name: str, ids: list[int]) -> str:\n",
    "        id = np.random.choice(0,1000)\n",
    "        while id in ids:\n",
    "            id = np.random.choice(0,1000)\n",
    "        return f'{name}_*{id}'\n",
    "    \n",
    "    def get_item(self, item_dict, path):\n",
    "        #path = [x.split('_*')[0] for x in path]\n",
    "        current = item_dict\n",
    "        while len(path) > 0:\n",
    "            ind = path.pop(0)\n",
    "            if type(ind) == list:\n",
    "                current = current[ind[0]][ind[1]]\n",
    "            else:\n",
    "                current = current[ind]\n",
    "        return current\n",
    "    \n",
    "    def get_endpoints(self) -> dict[\"JSONEndpoint\", any]:\n",
    "        return self.endpoints\n",
    "    \n",
    "    def find_all(self, query: any) -> dict[str, any]:\n",
    "        \"\"\"Returns the location and value of all data containing the query\"\"\"\n",
    "        matches = {}\n",
    "        for endpoint, val in self.endpoints.items():\n",
    "            if type(val) in [list, str]:\n",
    "                if query in val:\n",
    "                    path = endpoint.get_path()\n",
    "                    matches[endpoint] = val\n",
    "            elif type(query) in [float, int]:\n",
    "                if query == val:\n",
    "                    path = endpoint.get_path()\n",
    "                    matches[f'Location: root{path}'] = val\n",
    "        return matches\n",
    "    \n",
    "    def find_tags(self, query: str, return_endpoints=False) -> dict[str, any]:\n",
    "        matches = {}\n",
    "        for endpoint in self.endpoints:\n",
    "            path = endpoint.get_path()\n",
    "            if query in path:\n",
    "                node = endpoint.parent\n",
    "                while True:\n",
    "                    if type(node.name) == list:\n",
    "                        tag = node.name[1]\n",
    "                    else:\n",
    "                        tag = node.name\n",
    "                    if tag == query:\n",
    "                        break\n",
    "                    node = node.parent\n",
    "                \n",
    "                matches[node] = self.get_item(self.js, node.get_path())\n",
    "        return matches\n",
    "    \n",
    "    def find_neighbors_tag(self, query):\n",
    "        endpoints = self.find_tags(query)\n",
    "        neighbors = {}\n",
    "        for endpoint in endpoints:\n",
    "            if isinstance(endpoint, JSONVertex):\n",
    "                neighbors[endpoint.parent] = self.get_item(self.js, endpoint.parent.get_path())\n",
    "            else:\n",
    "                neighbors[endpoint.parent.parent] = self.get_item(self.js, endpoint.parent.parent.get_path())\n",
    "        return neighbors\n",
    "    \n",
    "    def find_neighbors_content(self, query):\n",
    "        endpoints = self.find_all(query)\n",
    "        neighbors = {}\n",
    "        for endpoint in endpoints:\n",
    "            neighbors[endpoint.parent.parent] = self.get_item(self.js, endpoint.parent.parent.get_path())\n",
    "        return\n",
    "    \n",
    "    def find_neighbors(self, vertex: \"JSONVertex | JSONEndpoint\") -> dict:\n",
    "        if isinstance(vertex, JSONVertex):\n",
    "            return self.get_item(self.js, vertex.parent.get_path())\n",
    "        elif isinstance(vertex, JSONEndpoint):\n",
    "            return self.get_item(self.js, vertex.parent.parent.get_path())\n",
    "        else:\n",
    "            raise TypeError('Vertex Must be a JSONVertex or JSONEndpoint instance')\n",
    "\n",
    "\n",
    "class JSONVertex:\n",
    "    children: list[str]\n",
    "    parent: \"JSONVertex | None\"\n",
    "    name: str | list[int, str]\n",
    "    def __init__(self, name, parent: \"JSONVertex | None\"):\n",
    "        self.parent = parent\n",
    "        self.name = name\n",
    "        pass\n",
    "\n",
    "    def get_path(self):\n",
    "        path = [self.name]\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            parent_name = node.parent.name\n",
    "            if type(parent_name) == list:\n",
    "                path = parent_name + path \n",
    "            else:\n",
    "                path = [parent_name] + path\n",
    "            node = node.parent\n",
    "        return path\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        path = self.get_path()\n",
    "        return f\"JSONEndpoint(root{path})\"\n",
    "\n",
    "\n",
    "class JSONEndpoint:\n",
    "    parent: JSONVertex\n",
    "    value: any\n",
    "    def __init__(self, value, parent):\n",
    "        self.parent = parent\n",
    "        self.value = value\n",
    "        pass\n",
    "\n",
    "    def get_path(self):\n",
    "        path = []\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            parent_name = node.parent.name\n",
    "            if type(parent_name) == list:\n",
    "                path = parent_name + path \n",
    "            else:\n",
    "                path = [parent_name] + path\n",
    "            node = node.parent\n",
    "        return path\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.parent.name}: {self.value}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        path = self.get_path()\n",
    "        return f\"JSONEndpoint(root{path})\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
