{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from json_parser import JSONGraph, JSONEndpoint, JSONVertex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_news_links(date: str) -> list[str]:\n",
    "    \"\"\"Returns links to featured GroundNews articles from the front page of\n",
    "    GroundNews on the inputted date\"\"\"\n",
    "    try:\n",
    "        date = ''.join(date.split('-'))\n",
    "        if len(date) != 8:\n",
    "            raise ValueError('Date must be in YYYY-MM-DD format')\n",
    "    except ValueError:\n",
    "        raise ValueError('Date must be in YYYY-MM-DD format')\n",
    "    \n",
    "    url = f\"https://web.archive.org/web/{date}/https://ground.news/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionRefusedError('Connection Error')\n",
    "    content = BeautifulSoup(response.text, \"html.parser\")\n",
    "    anchors = content.find_all(\"a\", href=True)\n",
    "    anchor_links = [x.attrs['href'] for x in anchors] \n",
    "    article_links = [x for x in anchor_links if 'article/' in x]\n",
    "    article_links = [x[x.find('https:'):] for x in article_links]\n",
    "    return article_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "old_dates = None\n",
    "try:\n",
    "    scraped_df = pd.read_csv('data/complete_article_data.csv')\n",
    "    old_dates = set(scraped_df['date'].values.tolist())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#CHANGE HERE FOR NEW URLS (don't use same dates)\n",
    "dates = ['2025-02-11', '2025-01-11', '2024-12-11', '2024-11-11', '2024-10-11',\n",
    "        '2024-09-11', '2024-08-11', '2024-07-11', '2024-06-11', '2024-05-11',\n",
    "        '2024-04-11', '2024-03-11', '2024-02-11', '2024-01-11']\n",
    "\n",
    "links = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through the dates list, grabbing available ground news article links from the ground news homepage on the input date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles from 2024-11-01 already scraped\n"
     ]
    }
   ],
   "source": [
    "for ind, date in enumerate(dates):\n",
    "    if old_dates:\n",
    "        if date in old_dates:\n",
    "            print(f'Articles from {date} already scraped')\n",
    "            continue\n",
    "    links.append(get_ground_news_links(dates[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1521, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ground.news/article/houthi-rebels-say-...</td>\n",
       "      <td>2024-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ground.news/article/tech-innovations-t...</td>\n",
       "      <td>2024-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ground.news/article/us-navy-helicopter...</td>\n",
       "      <td>2024-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ground.news/article/scientists-make-hu...</td>\n",
       "      <td>2024-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ground.news/article/archeologists-map-...</td>\n",
       "      <td>2024-02-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                url        date\n",
       "729         NaN  https://ground.news/article/houthi-rebels-say-...  2024-02-11\n",
       "730         NaN  https://ground.news/article/tech-innovations-t...  2024-02-11\n",
       "731         NaN  https://ground.news/article/us-navy-helicopter...  2024-02-11\n",
       "732         NaN  https://ground.news/article/scientists-make-hu...  2024-02-11\n",
       "733         NaN  https://ground.news/article/archeologists-map-...  2024-02-11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading df to check for already-scraped links\n",
    "current_gnews = pd.read_csv('data/ground_news_links.csv')\n",
    "seen_links = set(current_gnews['url'].values.tolist())\n",
    "\n",
    "link_col = []\n",
    "date_col = []\n",
    "#Adding unseen links to the ground_news_links csv\n",
    "for ind, link_group in enumerate(links):\n",
    "    date = dates[ind]\n",
    "    for link in link_group:\n",
    "        if link not in seen_links:\n",
    "            link_col.append(link)\n",
    "            date_col.append(date)\n",
    "\n",
    "gnews_link_df = pd.DataFrame()\n",
    "gnews_link_df['url'] = link_col\n",
    "gnews_link_df['date'] = date_col\n",
    "try:\n",
    "    old_gnews_links = pd.read_csv('data/ground_news_links.csv')\n",
    "    gnews_link_df = pd.concat([old_gnews_links, gnews_link_df], axis=0)\n",
    "except:\n",
    "    pass\n",
    "gnews_link_df.to_csv('data/ground_news_links.csv', index=False)\n",
    "\n",
    "print(gnews_link_df.shape)\n",
    "gnews_link_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2024-08-01',\n",
       " '2024-09-01',\n",
       " '2024-10-01',\n",
       " '2024-11-01',\n",
       " '2024-12-01',\n",
       " '2025-01-01',\n",
       " '2025-02-01'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnews_link_df = pd.read_csv('data/ground_news_links.csv')\n",
    "urls = []\n",
    "biases = []\n",
    "outlets = []\n",
    "titles = []\n",
    "dates = []\n",
    "old_queue = pd.read_csv('data/articles_to_scrape.csv')\n",
    "seen_dates = set(old_queue['date'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through ground news article links, grabbing all available urls to outside articles as well as recording the outlet and bias that url is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "def isolate_first_10(inp_str: str, start_str: str) -> dict:\n",
    "    \"\"\"Parses the groundnews javascript output into JSON\"\"\"\n",
    "    start_ind = inp_str.find(start_str) - 2\n",
    "    open_ind = inp_str.find(start_str) + len(start_str)\n",
    "    bracket_num = 0\n",
    "    started = False\n",
    "    for ind, char in enumerate(inp_str[open_ind:]):\n",
    "        if char == '[':\n",
    "            bracket_num += 1\n",
    "            started = True\n",
    "        if char == ']':\n",
    "            bracket_num -= 1\n",
    "        if bracket_num == 0 and started:\n",
    "            end_ind = ind + open_ind\n",
    "            break\n",
    "    exerpt = inp_str[start_ind:end_ind+1]\n",
    "    pattern = r'\\\\\"|\"\\\\'  # Match \\\\ before or after \"\n",
    "    replacement = '\"'\n",
    "\n",
    "    result = re.sub(pattern, replacement, exerpt)\n",
    "    pattern2 = r'\\\\\"'\n",
    "    result = re.sub(pattern, '\"', result)\n",
    "    result = f\"\"\"\n",
    "    {{\n",
    "        {result}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return json.loads(result)\n",
    "\n",
    "num_processed = 0\n",
    "\n",
    "for ind in range(gnews_link_df.shape[0]):\n",
    "    row = gnews_link_df.iloc[ind]\n",
    "    url = row['url']\n",
    "    date = row['date']\n",
    "    if date not in seen_dates:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            #Continuing if connection not established\n",
    "            if response.status_code != 200:\n",
    "                print(response.status_code)\n",
    "                raise ConnectionRefusedError('Connection Error')\n",
    "            #Getting html content\n",
    "            content = BeautifulSoup(response.text, \"html.parser\")\n",
    "            #print('script fine')\n",
    "            script_tags = content.find_all('script')\n",
    "            \n",
    "            js_raw = ''\n",
    "\n",
    "            #Isolating the script element containing the relevant data\n",
    "            for script in script_tags:\n",
    "                txt = script.text\n",
    "                if 'firstTenSources' in txt:\n",
    "                    js_raw = txt\n",
    "            tag_f = 'firstTenSources'\n",
    "\n",
    "            #Transforming text into a json object/dictionary\n",
    "            js = isolate_first_10(js_raw, tag_f)\n",
    "        except:\n",
    "            continue\n",
    "        #Appending data to lists\n",
    "        url_data = js['firstTenSources']\n",
    "        date = gnews_link_df['date'].values[ind] \n",
    "        num_processed += 1\n",
    "        for url_dict in url_data:\n",
    "            biases.append(url_dict['sourceInfo']['bias'])\n",
    "            outlets.append(url_dict['sourceInfo']['name'])\n",
    "            urls.append(url_dict['url'])\n",
    "            titles.append(url_dict['title'])\n",
    "            dates.append(date)\n",
    "        time.sleep(.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12679, 5)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_queue_df = pd.DataFrame()\n",
    "scrape_queue_df['url'] = urls\n",
    "scrape_queue_df['title'] = titles\n",
    "scrape_queue_df['outlet'] = outlets\n",
    "scrape_queue_df['bias'] = biases\n",
    "scrape_queue_df['date'] = dates\n",
    "\n",
    "scrape_queue_df.head()\n",
    "scrape_queue_combined = pd.concat([old_queue, scrape_queue_df], axis=0)\n",
    "scrape_queue_combined.drop_duplicates(subset=['url'])\n",
    "scrape_queue_combined.to_csv('data/articles_to_scrape.csv', index=False)\n",
    "scrape_queue_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3664"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=2.5)\n",
    "        if response.status_code != 200:\n",
    "            return 'Error'\n",
    "    except:\n",
    "        return 'Error'\n",
    "    content = BeautifulSoup(response.text, \"html.parser\")\n",
    "    all_p = content.find_all('p')\n",
    "    clean_text = [x.text for x in all_p]\n",
    "    full_text = ' '.join(clean_text)\n",
    "    return full_text\n",
    "\n",
    "scrape_queue_df = pd.read_csv('data/articles_to_scrape.csv')\n",
    "try:\n",
    "    pre_scraped_df = pd.read_csv('data/scraped_articles.csv')\n",
    "    scraped_urls = pre_scraped_df['url'].values.tolist()\n",
    "    scraped_text = pre_scraped_df['content'].values.tolist()\n",
    "except:\n",
    "    scraped_urls = []\n",
    "    scraped_text = []\n",
    "\n",
    "scraped_text_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 120.796875\n",
      "50 120.796875\n",
      "100 120.796875\n",
      "150 120.796875\n",
      "200 120.796875\n",
      "250 120.796875\n",
      "300 120.796875\n",
      "350 120.796875\n",
      "400 120.796875\n",
      "450 120.796875\n",
      "500 120.796875\n",
      "550 120.796875\n",
      "600 120.796875\n",
      "650 120.796875\n",
      "700 120.796875\n",
      "750 120.796875\n",
      "800 120.796875\n",
      "850 120.796875\n",
      "900 120.796875\n",
      "950 120.796875\n",
      "1000 120.796875\n",
      "1050 120.796875\n",
      "1100 120.796875\n",
      "1150 120.796875\n",
      "1200 120.796875\n",
      "1250 120.796875\n",
      "1300 120.796875\n",
      "1350 120.796875\n",
      "1400 120.796875\n",
      "1450 120.8125\n",
      "1500 120.8125\n",
      "1550 120.8125\n",
      "1600 120.8125\n",
      "1650 120.8125\n",
      "1700 120.8125\n",
      "1750 120.8125\n",
      "1800 120.8125\n",
      "1850 120.8125\n",
      "1900 120.8125\n",
      "1950 120.8125\n",
      "2000 120.8125\n",
      "2050 120.8125\n",
      "2100 120.8125\n",
      "2150 120.8125\n",
      "2200 120.8125\n",
      "2250 120.8125\n",
      "2300 120.8125\n",
      "2350 120.8125\n",
      "2400 120.8125\n",
      "2450 120.8125\n",
      "2500 120.828125\n",
      "2550 120.828125\n",
      "2600 120.828125\n",
      "2650 120.828125\n",
      "2700 120.828125\n",
      "2750 120.828125\n",
      "2800 120.828125\n",
      "2850 120.828125\n",
      "2900 120.828125\n",
      "2950 120.828125\n",
      "3000 120.828125\n",
      "3050 120.828125\n",
      "3100 120.828125\n",
      "3150 120.828125\n",
      "3200 120.828125\n",
      "3250 120.828125\n",
      "3300 120.828125\n",
      "3350 120.84375\n",
      "3400 120.84375\n",
      "3450 120.84375\n",
      "3500 120.84375\n",
      "3550 120.84375\n",
      "3600 120.84375\n",
      "3650 120.84375\n",
      "3700 120.84375\n",
      "3750 120.84375\n",
      "3800 120.84375\n",
      "3850 120.84375\n",
      "3900 120.84375\n",
      "3950 120.84375\n",
      "4000 120.84375\n",
      "4050 120.84375\n",
      "4100 120.84375\n",
      "4150 120.859375\n",
      "4200 120.859375\n",
      "4250 120.859375\n",
      "4300 120.859375\n",
      "4350 120.859375\n",
      "4400 120.859375\n",
      "4450 120.859375\n",
      "4500 120.859375\n",
      "4550 120.859375\n",
      "4600 120.859375\n",
      "4650 120.875\n",
      "4700 120.875\n",
      "4750 120.875\n",
      "4800 120.875\n",
      "4850 120.875\n",
      "4900 120.875\n",
      "4950 120.875\n",
      "5000 120.875\n",
      "5050 120.875\n",
      "5100 120.875\n",
      "5150 120.890625\n",
      "5200 120.890625\n",
      "5250 120.890625\n",
      "5300 120.890625\n",
      "5350 120.890625\n",
      "5400 120.890625\n",
      "5450 120.890625\n",
      "5500 120.890625\n",
      "5550 120.890625\n",
      "5600 120.890625\n",
      "5650 120.90625\n",
      "5700 120.90625\n",
      "5750 120.90625\n",
      "5800 120.90625\n",
      "5850 120.90625\n",
      "5900 120.90625\n",
      "5950 120.90625\n",
      "6000 120.90625\n",
      "6050 120.921875\n",
      "6100 120.921875\n",
      "6150 120.921875\n",
      "6200 120.921875\n",
      "6250 120.921875\n",
      "6300 120.921875\n",
      "6350 120.921875\n",
      "6400 120.921875\n",
      "6450 120.921875\n",
      "6500 120.9375\n",
      "6550 120.9375\n",
      "6600 120.9375\n",
      "6650 120.9375\n",
      "6700 120.9375\n",
      "6750 120.9375\n",
      "6800 120.9375\n",
      "6850 120.953125\n",
      "6900 120.953125\n",
      "6950 120.953125\n",
      "7000 120.953125\n",
      "7050 120.953125\n",
      "7100 120.953125\n",
      "7150 120.953125\n",
      "7200 120.96875\n",
      "7250 120.96875\n",
      "7300 120.96875\n",
      "7350 120.96875\n",
      "7400 120.96875\n",
      "7450 120.96875\n",
      "7500 120.96875\n",
      "7550 120.984375\n",
      "7600 120.984375\n",
      "7650 120.984375\n",
      "7700 121.453125\n",
      "7750 125.140625\n",
      "7800 128.3125\n",
      "7850 133.546875\n",
      "7900 138.484375\n",
      "7950 142.34375\n",
      "8000 145.859375\n",
      "8050 145.859375\n",
      "8100 145.859375\n",
      "8150 145.859375\n",
      "8200 145.859375\n",
      "8250 145.859375\n",
      "8300 145.921875\n",
      "8350 145.96875\n",
      "8400 145.96875\n",
      "8450 145.984375\n",
      "8500 145.984375\n",
      "8550 145.984375\n",
      "8600 145.984375\n",
      "8650 145.984375\n",
      "8700 145.984375\n",
      "8750 146.140625\n",
      "8800 146.15625\n",
      "8850 146.25\n",
      "8900 146.25\n",
      "8950 146.28125\n",
      "9000 146.296875\n",
      "9050 146.296875\n",
      "9100 146.296875\n",
      "9150 146.34375\n",
      "9200 146.421875\n",
      "9250 146.421875\n",
      "9300 146.625\n",
      "9350 146.625\n",
      "9400 146.625\n",
      "9450 146.640625\n",
      "9500 146.640625\n",
      "9550 146.640625\n",
      "9600 146.640625\n",
      "9650 146.640625\n",
      "9700 148.984375\n",
      "9750 152.59375\n",
      "9800 155.171875\n",
      "9850 159.109375\n",
      "9900 163.875\n",
      "9950 168.078125\n",
      "10000 172.625\n",
      "10050 178.109375\n",
      "10100 183.734375\n",
      "10150 189.046875\n",
      "10200 193.015625\n",
      "10250 199.015625\n",
      "10300 204.96875\n",
      "10350 209.75\n",
      "10400 215.90625\n",
      "10450 221.484375\n",
      "10500 227.328125\n",
      "10550 234.0625\n",
      "10600 239.65625\n",
      "10650 244.90625\n",
      "10700 250.21875\n",
      "10750 255.078125\n",
      "10800 261.671875\n",
      "10850 268.34375\n",
      "10900 274.96875\n",
      "10950 280.296875\n",
      "11000 286.375\n",
      "11050 292.828125\n",
      "11100 298.90625\n",
      "11150 303.203125\n",
      "11200 308.8125\n",
      "11250 313.703125\n",
      "11300 319.671875\n",
      "11350 326.015625\n",
      "11400 331.0625\n",
      "11450 336.65625\n",
      "11500 341.25\n",
      "11550 344.515625\n",
      "11600 348.28125\n",
      "11650 351.40625\n",
      "11700 354.15625\n",
      "11750 358.0\n",
      "11800 361.0625\n",
      "11850 364.265625\n",
      "11900 366.9375\n",
      "11950 369.859375\n",
      "12000 372.953125\n",
      "12050 376.328125\n",
      "12100 379.671875\n",
      "12150 382.5\n",
      "12200 385.4375\n",
      "12250 389.390625\n",
      "12300 392.265625\n",
      "12350 394.8125\n",
      "12400 396.453125\n",
      "12450 396.46875\n",
      "12500 396.46875\n",
      "12550 396.46875\n",
      "12600 396.46875\n",
      "12650 396.484375\n"
     ]
    }
   ],
   "source": [
    "#Iterates through unseen URLs, scraping their text and occasionally updating the CSV\n",
    "for ind, url in enumerate(scrape_queue_df['url'].values):\n",
    "    if ind % 50 == 0:\n",
    "        print(ind)\n",
    "    if url not in scraped_urls:\n",
    "        scraped_text.append(scrape_url(url))\n",
    "        scraped_urls.append(url)\n",
    "    else:\n",
    "        continue\n",
    "    if ind % 50 == 0 or ind == scrape_queue_df.shape[0]-1:\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['url'] = scraped_urls\n",
    "        temp_df['content'] = scraped_text\n",
    "        temp_df.to_csv('data/scraped_articles.csv', index=False)\n",
    "        scraped_text_df = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out URLS that could not be scraped from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_article_data = pd.read_csv('data/complete_article_data.csv')\n",
    "complete_articles = []\n",
    "complete_urls = []\n",
    "for ind, article in enumerate(scraped_text_df['content'].values):\n",
    "    try:\n",
    "        if len(article.split()) > 50:\n",
    "            complete_articles.append(article)\n",
    "            complete_urls.append(scraped_text_df['url'].values[ind])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "complete_df = pd.DataFrame()\n",
    "complete_df['url'] = complete_urls\n",
    "complete_df['content'] = complete_articles\n",
    "complete_df = scrape_queue_df.merge(complete_df, how='inner', on='url')\n",
    "merged_article_data = pd.concat([old_article_data, complete_df], axis=0)\n",
    "merged_article_data.drop_duplicates(subset=['url'], inplace=True)\n",
    "merged_article_data.to_csv('data/complete_article_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON parser class if import did not work for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONGraph:\n",
    "\n",
    "    def __init__(self, js: dict):\n",
    "        self.js = js\n",
    "        self.roots = list(js.keys())\n",
    "        queue = [self.add_vert(item, None) for item in self.roots]\n",
    "\n",
    "        endpoints = {}\n",
    "        endpoint_parents = {}\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            node_path = node.get_path()\n",
    "            node_val = self.get_item(js, node_path)\n",
    "            #Handling endpoints\n",
    "            if type(node_val) == dict:\n",
    "                children = node_val.keys()\n",
    "                for child in children:\n",
    "                    queue.append(self.add_vert(child, node))                \n",
    "            elif type(node_val) == list:\n",
    "                for ind, item in enumerate(node_val):\n",
    "                    if type(item) == dict:\n",
    "                        children = item.keys()\n",
    "                        for child in children:\n",
    "                            queue.append(self.add_vert([ind, child], node))\n",
    "                    else: \n",
    "                        endpoint = self.add_endpoint([ind, item], node)\n",
    "                        endpoints[endpoint] = node_val         \n",
    "            else:\n",
    "                endpoint = self.add_endpoint(node_val, node)\n",
    "                endpoints[endpoint] = node_val\n",
    "        self.endpoints = endpoints\n",
    "        return\n",
    "\n",
    "    def add_vert(self, name, parent: \"JSONVertex | None\") -> \"JSONVertex\":\n",
    "        return JSONVertex(name, parent)\n",
    "\n",
    "    def add_endpoint(self, value: any, parent: \"JSONVertex\") -> \"JSONEndpoint\":\n",
    "        return JSONEndpoint(value, parent)\n",
    "\n",
    "    def get_id(self, name: str, ids: list[int]) -> str:\n",
    "        id = np.random.choice(0,1000)\n",
    "        while id in ids:\n",
    "            id = np.random.choice(0,1000)\n",
    "        return f'{name}_*{id}'\n",
    "    \n",
    "    def get_item(self, item_dict, path):\n",
    "        #path = [x.split('_*')[0] for x in path]\n",
    "        current = item_dict\n",
    "        while len(path) > 0:\n",
    "            ind = path.pop(0)\n",
    "            if type(ind) == list:\n",
    "                current = current[ind[0]][ind[1]]\n",
    "            else:\n",
    "                current = current[ind]\n",
    "        return current\n",
    "    \n",
    "    def get_endpoints(self) -> dict[\"JSONEndpoint\", any]:\n",
    "        return self.endpoints\n",
    "    \n",
    "    def find_all(self, query: any) -> dict[str, any]:\n",
    "        \"\"\"Returns the location and value of all data containing the query\"\"\"\n",
    "        matches = {}\n",
    "        for endpoint, val in self.endpoints.items():\n",
    "            if type(val) in [list, str]:\n",
    "                if query in val:\n",
    "                    path = endpoint.get_path()\n",
    "                    matches[endpoint] = val\n",
    "            elif type(query) in [float, int]:\n",
    "                if query == val:\n",
    "                    path = endpoint.get_path()\n",
    "                    matches[f'Location: root{path}'] = val\n",
    "        return matches\n",
    "    \n",
    "    def find_tags(self, query: str, return_endpoints=False) -> dict[str, any]:\n",
    "        matches = {}\n",
    "        for endpoint in self.endpoints:\n",
    "            path = endpoint.get_path()\n",
    "            if query in path:\n",
    "                node = endpoint.parent\n",
    "                while True:\n",
    "                    if type(node.name) == list:\n",
    "                        tag = node.name[1]\n",
    "                    else:\n",
    "                        tag = node.name\n",
    "                    if tag == query:\n",
    "                        break\n",
    "                    node = node.parent\n",
    "                \n",
    "                matches[node] = self.get_item(self.js, node.get_path())\n",
    "        return matches\n",
    "    \n",
    "    def find_neighbors_tag(self, query):\n",
    "        endpoints = self.find_tags(query)\n",
    "        neighbors = {}\n",
    "        for endpoint in endpoints:\n",
    "            if isinstance(endpoint, JSONVertex):\n",
    "                neighbors[endpoint.parent] = self.get_item(self.js, endpoint.parent.get_path())\n",
    "            else:\n",
    "                neighbors[endpoint.parent.parent] = self.get_item(self.js, endpoint.parent.parent.get_path())\n",
    "        return neighbors\n",
    "    \n",
    "    def find_neighbors_content(self, query):\n",
    "        endpoints = self.find_all(query)\n",
    "        neighbors = {}\n",
    "        for endpoint in endpoints:\n",
    "            neighbors[endpoint.parent.parent] = self.get_item(self.js, endpoint.parent.parent.get_path())\n",
    "        return\n",
    "    \n",
    "    def find_neighbors(self, vertex: \"JSONVertex | JSONEndpoint\") -> dict:\n",
    "        if isinstance(vertex, JSONVertex):\n",
    "            return self.get_item(self.js, vertex.parent.get_path())\n",
    "        elif isinstance(vertex, JSONEndpoint):\n",
    "            return self.get_item(self.js, vertex.parent.parent.get_path())\n",
    "        else:\n",
    "            raise TypeError('Vertex Must be a JSONVertex or JSONEndpoint instance')\n",
    "\n",
    "\n",
    "class JSONVertex:\n",
    "    children: list[str]\n",
    "    parent: \"JSONVertex | None\"\n",
    "    name: str | list[int, str]\n",
    "    def __init__(self, name, parent: \"JSONVertex | None\"):\n",
    "        self.parent = parent\n",
    "        self.name = name\n",
    "        pass\n",
    "\n",
    "    def get_path(self):\n",
    "        path = [self.name]\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            parent_name = node.parent.name\n",
    "            if type(parent_name) == list:\n",
    "                path = parent_name + path \n",
    "            else:\n",
    "                path = [parent_name] + path\n",
    "            node = node.parent\n",
    "        return path\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        path = self.get_path()\n",
    "        return f\"JSONEndpoint(root{path})\"\n",
    "\n",
    "\n",
    "class JSONEndpoint:\n",
    "    parent: JSONVertex\n",
    "    value: any\n",
    "    def __init__(self, value, parent):\n",
    "        self.parent = parent\n",
    "        self.value = value\n",
    "        pass\n",
    "\n",
    "    def get_path(self):\n",
    "        path = []\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            parent_name = node.parent.name\n",
    "            if type(parent_name) == list:\n",
    "                path = parent_name + path \n",
    "            else:\n",
    "                path = [parent_name] + path\n",
    "            node = node.parent\n",
    "        return path\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.parent.name}: {self.value}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        path = self.get_path()\n",
    "        return f\"JSONEndpoint(root{path})\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
